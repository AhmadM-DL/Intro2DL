{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction2DL: Part2 Artificial Neural Networks with SKlearn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMDLDMQn6JZIXHpJvnqRqGs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadM-DL/Intro2DL/blob/main/Introduction2DL_Part2_Artificial_Neural_Networks_with_SKlearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSCERioIQBAi"
      },
      "source": [
        "#Artificial Neural Networks with SciKit Learn\n",
        "<!-- \n",
        "By: Ahmad Mustapha - Machine Learning researcher @ AUB \n",
        "LinkedIn: ahmad-mustapha-ml\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbTZZUMU_1w"
      },
      "source": [
        "[First we tinker with the NN](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.81394&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&playButton_hide=false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGinMij2Ch-A"
      },
      "source": [
        "### Machine Learning Task\n",
        "\n",
        "In this Lab session we will dive into Deep Learning using Scikit Learn Multi-Layer Perceptron (MLP) classes.\n",
        "\n",
        "We will be using [Pima Indians Diabetes Dataset](https://https://www.openml.org/d/37). The dataset task is to classify wither a female patient from the pima indians natives shows signs of diabetes according to World Health Organization criteria (i.e., if the 2 hour post-load plasma glucose was at least 200 mg/dl). All the patients where at least 21 of age.\n",
        "\n",
        "The dataset contains 768 record each of 8 features/predictors along with the binary target meaning Diabetes/No Diabetes. The predictors are:\n",
        "  1. Number of times pregnant\n",
        "  2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "  3. Diastolic blood pressure (mm Hg)\n",
        "  4. Triceps skin fold thickness (mm)\n",
        "  5. 2-Hour serum insulin (mu U/ml)\n",
        "  6. Body mass index (weight in kg/(height in m)^2)\n",
        "  7. Diabetes pedigree function\n",
        "  8. Age (years)\n",
        "\n",
        "The dataset is unbalanced as 500 recordes are labled as negative and 268 labeled as positive.\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ZYEAEc-YXj"
      },
      "source": [
        "# Get Diabetes Data\n",
        "!wget https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYG2UFvG4M8D"
      },
      "source": [
        "# Imports\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo5s6IuEFOfw"
      },
      "source": [
        "### Exploring/ Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giCL88pf6Kuc"
      },
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('diabetes.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxSAo6ngFUjy"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpKsoz0jFWhD"
      },
      "source": [
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kv6p8WBHOAk"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "# Seperate Target and Predictors, get as numpy array\n",
        "features, targets = df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
        "\n",
        "# Scale Features (Scaling is importent for ANNs)\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Split Train/Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.30, random_state=40)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k7mefdmFzVt"
      },
      "source": [
        "# The data manifold \n",
        "from sklearn.manifold import TSNE\n",
        "#from sklearn.decomposition import PCA\n",
        "tsne = TSNE(n_components=2, perplexity=30)\n",
        "reduced_features = tsne.fit_transform(features)\n",
        "\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "p_reduced_features = [row for i,row in enumerate(reduced_features) if targets[i]==1]\n",
        "n_reduced_features = [row for i,row in enumerate(reduced_features) if targets[i]==0]\n",
        "\n",
        "plt.scatter([f1 for (f1, _) in n_reduced_features], [f2 for (_, f2) in n_reduced_features], label=\"Diabetes Negative\", c=\"green\")\n",
        "plt.scatter([f1 for (f1, _) in p_reduced_features], [f2 for (_, f2) in p_reduced_features], label=\"Diabetes Positive\", c=\"red\")\n",
        "\n",
        "plt.title(\"Pima Indians Diabetes Data Manifold\")\n",
        "plt.xlabel(\"tsne_1\")\n",
        "plt.xlabel(\"tsne_2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0wAY8xZKYam"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "In this section we will initilize and train SKlearn's [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). We decided to start this to ease the learning curve. However SKlearn APIs obsecure alot of the Deep Learning underlayings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyJfkl7TTovL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc-AXKiM-y5X"
      },
      "source": [
        "# ANN Classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(4), activation='relu', solver='adam', max_iter=500)\n",
        "\n",
        "# Train\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Test\n",
        "predict_train = mlp.predict(X_train)\n",
        "predict_test = mlp.predict(X_test)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFnUUbkDBy6i"
      },
      "source": [
        "print(classification_report(y_test, predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV4Bm3BTL9_I"
      },
      "source": [
        "### Using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anycPtQBR8Ce"
      },
      "source": [
        "<img src=\"https://keras.io/img/logo.png\" width=\"330\"></img>\n",
        "\n",
        "Keras is a high-level Deep Learning library. According to thier website: \n",
        "\n",
        "> Deep learning for humans. Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. It also has extensive documentation and developer guides.\n",
        "\n",
        "Keras is build on Tensorflow (A low-level DL Library)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MQosRIUVMor"
      },
      "source": [
        "Keras offers different neural networks building blocks. Those are available through the [keras.models](https://keras.io/api/models/model/) module. The [Sequential](https://keras.io/api/models/sequential/) model is used to stack different layers lineary in which gradients and computations can flow from one layer to another. In this lecture we will start by the [Dense](https://keras.io/api/layers/core_layers/dense/) layer building block. The Dense layer is used to create Fully Connected Layers. Remeber that MLP is a linear stack of fully connected layers. \n",
        "\n",
        "The Dense constructor takes on multiple parameters the following are of interest for now:\n",
        "*   unites: The number of unites the layer include\n",
        "*   acitivation: The [activation](https://keras.io/api/layers/activations/) function to apply after crossing input with weights [relu, sigmoid, tanh]\n",
        "*   input_dim: The number size, use only for the first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M26U85o6MBWa"
      },
      "source": [
        "# Imports \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsmaTIHUMLjU"
      },
      "source": [
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmk17JQKY_0L"
      },
      "source": [
        "We can understand the built model using Keras models *`Summary`* or Keras Utility function `keras.utils.vis_utils.plot_model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzSynDEhY_Iq"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0msZ8p_ZiRu"
      },
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AtiEdQCaDPz"
      },
      "source": [
        "We prepare the model for training using the model *compile* method. Using it we add more properties for the training process. Most of intrest for now:\n",
        "*   Loss Function: The loss function used to compute the error between the model predictions and the ground truth.\n",
        "*   Optimizer: The optimization method to use like stochastic gradient decent (SGD) and adaptive moment estimation (Adam)\n",
        "*   Metrics: The metrics we want to report. For classification we report accuraccy. For regression might report mean square error (MSE). ..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbMHgmd9M47b"
      },
      "source": [
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htR0hmovb-oW"
      },
      "source": [
        "We finally call the model's `fit` method to train it over the data. `fit` takes on the following parameters:\n",
        "*   x: Training Data (predictors)\n",
        "*   y: Training Ground Truth (targets)\n",
        "*   epochs: The number of times the model have to pass over hte entire dataset\n",
        "*   batch_size: The number of instance to train the model on at a time. The batch size is an importent parameter. By using batches we can train our models on huge datasets because we are loading a small sample of the records to the computing device (CPU, GPU) memory. When a batch of records it passed forward to the model. The loss is computed for each record averaged and fed back to the model to update the weights through backpropagation.\n",
        "* callbacks: An array of callback functions. A callback is a function that fires during the training process according to a predifined trigger. You don't need to worry about it alot as we will be using build functions. On function of interest is TensorBaord callback. It takes the metrics reported by the model (see compile above) and logs them into a certain directory. We later can view the logs using Tensorboard.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5gYqV80eHuz"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\"./runs\", histogram_freq=1)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHGi6WcqNR93"
      },
      "source": [
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=500, batch_size=200, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plxQdSrpNsx2"
      },
      "source": [
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntw0Y-jReeb7"
      },
      "source": [
        "%tensorboard --logdir \"./runs\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}